# Users can import user customized yaml files or library provided default yaml files
#includes: ['/configs/line_graph_construction.yaml']
  # - $/configs/defaults.yaml
  # In the above example path, '$/' indicates the library directory which contains the config folder


preprocessing_args:
  word_emb_size: 300
  min_word_freq: 1

model_args:
  graph_construction_name: "line_graph"
  graph_initialization_name: "defaults"
  graph_embedding_name: "graphsage"
  decoder_name: "stdrnn"
  num_class: 8
  tag_dropout: 0.5
  rnn_dropout: 0.33
  use_gnn: True
  init_hidden_size: 300
  hidden_size: 128

  graph_construction_args:
    graph_construction_share:
      root_dir: '../conll/'
      topology_subdir: 'LineGraph'
      thread_number: 10
      pre_word_emb_file: null

    nlp_processor_args:
      name: "stanza"
      args:
          annotators: ["tokenize,ssplit,pos,depparse"]
          corenlp_dir: "./corenlp"
          endpoint: "http://localhost:9002"
          memory: "4G"
          properties:
            tokenize.options: 
              splitHyphenated: False
              normalizeParentheses: False
              normalizeOtherBrackets: False
            tokenize.whitespace: True
            ssplit.isOneSentence: True

    graph_construction_private:
      edge_strategy: 'homogeneous'
      merge_strategy: 'tailhead'
      sequential_link: true
      as_node: false

  graph_initialization_args:
    input_size: 300
    hidden_size: 300
    init_hidden_size: 300
    word_dropout: 0.5
    rnn_dropout: 0.33
    fix_bert_emb: false
    fix_word_emb: true
    embedding_style:
      single_token_item: true
      emb_strategy: "w2v_bilstm"
      num_rnn_layers: 1
      bert_model_name: null
      bert_lower_case: null


  graph_embedding_args:
    graph_embedding_share:
      gnn_num_layers: 1
      input_size: 300
      init_hidden_size: 300
      hidden_size: 128
      output_size: 300
      direction_option: "bi_fuse"
      feat_drop: 0.3            


training_args:
  batch_size: 100 # batch size
  epochs: 100 # number of maximal training epochs
  grad_clipping: 10
  early_stop_metric: 'BLEU_4'
  patience: 10
  lr: 0.01 # learning rate
  lr_patience: 2
  lr_reduce_factor: 0.5
  weight_decay: 5e-5


checkpoint_args:
  out_dir: '../out/graphsage_bi_fuse_line_graph_ckpt'


env_args:
  seed: 1234
  gpu: 0
  no_cuda: false
  epochs: 5


